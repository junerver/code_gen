# 后端架构

<cite>
**本文档引用的文件**  
- [chat.post.ts](file://server/api/chat.post.ts)
- [template-gen.ts](file://server/core/prompt/template-gen.ts)
- [mcp-tools.ts](file://server/core/tools/mcp-tools.ts)
- [local-tools.ts](file://server/core/tools/local-tools.ts)
- [model.ts](file://server/utils/model.ts)
- [chat.ts](file://shared/types/api/chat.ts)
- [model.ts](file://shared/types/model.ts)
</cite>

## 目录
1. [简介](#简介)
2. [核心API入口：chat.post.ts](#核心api入口chatpostts)
3. [提示词工程：template-gen.ts](#提示词工程template-gents)
4. [工具系统：MCP与本地工具](#工具系统mcp与本地工具)
5. [模型抽象层：model.ts](#模型抽象层modelts)
6. [服务层设计与H3事件处理](#服务层设计与h3事件处理)
7. [总结](#总结)

## 简介
本系统旨在构建一个智能化的代码生成后端架构，集成AI模型、提示词工程、工具调用与多模型支持能力。系统通过H3框架接收前端请求，利用AI SDK执行流式文本生成，并结合MCP（Model Context Protocol）工具与本地工具扩展AI能力，最终实现高质量、规范化的代码输出。整体架构强调解耦、可扩展性与类型安全。

## 核心API入口：chat.post.ts

`chat.post.ts` 是系统的主API入口，负责接收来自前端的POST请求，协调AI模型调用、工具集成与提示词注入，最终返回流式代码生成结果。该文件采用 `defineLazyEventHandler` 实现延迟初始化，提升服务启动效率。

其核心流程如下：
1. 初始化MCP工具与本地工具
2. 解析请求体中的模型、消息与温度参数
3. 调用 `streamText` 启动AI流式响应
4. 注入系统提示词、工具集与模型实例
5. 返回兼容UI的消息流

该API通过 `readBody<ChatRequest>` 进行类型安全的请求解析，确保输入符合预定义接口规范。

**Section sources**
- [chat.post.ts](file://server/api/chat.post.ts#L1-L43)
- [chat.ts](file://shared/types/api/chat.ts#L1-L17)

## 提示词工程：template-gen.ts

`template-gen.ts` 定义了引导AI生成规范化代码的核心提示词逻辑。该提示词采用中文编写，确保AI理解任务目标与处理规则。

提示词设计包含以下关键要素：
- **角色定位**：明确AI为“代码生成引擎”，输出可执行代码，禁止解释性内容
- **模板列表**：声明支持的前后端与数据库模板类型
- **工作流程**：定义从需求解析、上下文获取、模板加载到渲染输出的完整流程
- **关键处理规则**：重点强调Velocity模板语法的处理方式，尤其是字面输出块 `#[[...]]#` 的优先级处理
- **错误预防机制**：规定缺失变量处理、循环验证与嵌套访问检查
- **质量保证要求**：确保占位符替换完整、循环遍历完全、格式保持一致

特别地，系统要求AI在处理 `#[[...]]#` 块时必须原样输出内容并移除标记，这是保证生成代码语法正确性的关键。

**Section sources**
- [template-gen.ts](file://server/core/prompt/template-gen.ts#L1-L237)

## 工具系统：MCP与本地工具

系统通过MCP工具与本地工具增强AI的能力边界，使其能够调用外部服务与执行特定逻辑。

### MCP工具（mcp-tools.ts）
MCP工具基于Model Context Protocol协议，通过标准输入输出与外部服务通信。当前系统集成了两类MCP服务：
- **模板服务**：用于获取指定名称的Velocity模板内容
- **MySQL服务**：用于根据数据表名生成模板上下文

工具初始化过程中，系统通过Zod定义严格的输入参数校验规则，确保调用安全。例如，`get_template_content` 仅接受预定义的模板名称枚举值。

MCP客户端通过 `uv` 命令启动，运行于独立进程，实现与主服务的解耦。

### 本地工具（local-tools.ts）
本地工具为内嵌于服务的轻量级功能模块。当前仅实现一个示例工具 `dateTime`，用于返回当前日期。

本地工具通过 `ai` SDK的 `tool` 函数定义，包含描述、输入模式与执行函数。未来可扩展更多实用工具，如文件操作、代码校验等。

**Section sources**
- [mcp-tools.ts](file://server/core/tools/mcp-tools.ts#L1-L93)
- [local-tools.ts](file://server/core/tools/local-tools.ts#L1-L19)

## 模型抽象层：model.ts

`model.ts` 实现了对不同LLM提供商的统一抽象，屏蔽底层差异，实现模型解耦。

系统支持以下模型提供商：
- **硅基流动（Siliconflow）**
- **Ollama**
- **DeepSeek**
- **阿里百炼（Bailian）**

核心设计如下：
1. 为每个提供商创建兼容OpenAI接口的客户端
2. 定义 `AvailableModels` 常量，声明所有可用模型的ID、名称、提供商与中间件配置
3. 通过 `createLanguageModel` 函数根据配置动态创建模型实例
4. 使用 `customProvider` 构建统一模型提供器
5. 导出类型安全的 `llmProvider` 函数，供外部按名称获取模型

该设计支持中间件扩展，如 `extractReasoningMiddleware` 可用于提取AI的思考过程。

**Section sources**
- [model.ts](file://server/utils/model.ts#L1-L121)
- [model.ts](file://shared/types/model.ts#L1-L148)

## 服务层设计与H3事件处理

系统采用分层架构设计，各层职责清晰：
- **API层**：`chat.post.ts` 处理HTTP请求与响应
- **核心逻辑层**：`prompt/` 与 `tools/` 提供业务能力
- **工具层**：`utils/` 提供通用函数
- **共享类型层**：`shared/types/` 定义跨层类型

H3事件处理机制采用 `defineLazyEventHandler` + `defineEventHandler` 双层结构：
- 外层 `defineLazyEventHandler` 实现工具的延迟初始化，避免服务启动时阻塞
- 内层 `defineEventHandler` 处理每次请求，复用已初始化的工具实例

此设计兼顾性能与可维护性，是Nuxt 3推荐的服务端处理模式。

**Section sources**
- [chat.post.ts](file://server/api/chat.post.ts#L1-L43)
- [model.ts](file://server/utils/model.ts#L1-L121)

## 总结
本后端架构通过模块化设计实现了AI代码生成系统的高内聚与低耦合。`chat.post.ts` 作为核心入口，整合了AI模型、提示词、工具系统三大能力；`template-gen.ts` 通过精细化提示词设计确保输出规范；MCP与本地工具扩展了AI的功能边界；`model.ts` 实现了多模型提供商的统一接入。整体架构为开发者提供了清晰的扩展路径，支持新增模型、工具与提示词策略，具备良好的可维护性与可扩展性。